# -*- coding: utf-8 -*-
"""temple_predictions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wkPNqniHVAbVPnJNyuSY_713IUdHTuXo
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Loading data (assuming the data is loaded correctly)
growth_data = pd.read_csv("growth_data.csv")
temple_ward_data = pd.read_csv("temple_and_ward_data.csv")

# Strip spaces from column names
growth_data.columns = growth_data.columns.str.strip()
temple_ward_data.columns = temple_ward_data.columns.str.strip()

# Merge the datasets on 'Country' column
data = growth_data.merge(temple_ward_data, on='Country', how='left')

# Define features and target
features = ['Members', 'Population', 'LDS', '2013', 'Change', 'Growth()', 'oftotalgrowth', 'Missions', 'Districts', 'Stakes', 'Wards', 'Branches', 'FSC']
target = 'Total'  # Predicting the total number of temples

# Clean the data again to replace any special characters or string representations
for col in features:
    data[col] = data[col].replace({'\s+': '', ' −': '-', '−': '-', 'NaN': ''}, regex=True)  # Replace non-standard minus signs and spaces
    data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert to numeric, turning errors into NaNs

# Handle NaN values by either dropping or imputing them
data = data.dropna(subset=features + [target])  # Drop rows where any NaN values exist in the features or target

# Define the features and target
X = data[features]
y = data[target]

# Step 1: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Store the indices of X_test before standardization
test_indices = X_test.index

# Step 2: Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 3: Define and compile the regression model
model = keras.Sequential([
    layers.InputLayer(input_shape=(X_train.shape[1],)),

    # More layers with more neurons
    layers.Dense(256, activation='relu'),  # Added a second hidden layer
    layers.Dense(128, activation='relu'),  # Third hidden layer
    layers.Dense(64, activation='relu'),   # Fourth hidden layer
    layers.Dense(32, activation='relu'),   # Fifth hidden layer
    layers.Dense(1)  # Output layer for regression
])

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Step 4: Train the model

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Step 5: Make predictions for the total number of temples
y_pred = model.predict(X_test)
# Step 6: Compare predicted vs actual values
comparison = pd.DataFrame({
    'Country': data.loc[test_indices, 'Country'],  # Use saved indices for the countries
    'Actual Temples': y_test,
    'Predicted Temples': y_pred.flatten()
})

# Round predictions and actual values to the nearest integer
comparison['Actual Temples'] = comparison['Actual Temples'].round()
comparison['Predicted Temples'] = comparison['Predicted Temples'].round()

# Calculate the difference between actual and predicted temples
comparison['Difference'] = comparison['Actual Temples'] - comparison['Predicted Temples']

# Filter countries where the actual number of temples is greater than the predicted number
countries_needing_more_temples = comparison[comparison['Difference'] < 0]
countries_with_more_temples = comparison[comparison['Difference'] > 0]

# Display the countries needing more temples
print("Countries that may need more temples:")
print(countries_needing_more_temples[['Country', 'Actual Temples', 'Predicted Temples', 'Difference']])

print("\nCountries with more temples:")
print(countries_with_more_temples[['Country', 'Actual Temples', 'Predicted Temples', 'Difference']])